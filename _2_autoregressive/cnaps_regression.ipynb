{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryees\\anaconda3\\envs\\pml\\Lib\\site-packages\\torch\\__init__.py:613: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "     torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 10\n",
    "\n",
    "hidden_size = 128  # Number of neurons in each hidden layer of base model and number of ouput of adaptive network\n",
    "num_output_adaptive = hidden_size\n",
    "num_input_setEncoderAr = hidden_size\n",
    "\n",
    "num_output_setEncoderContext = 128\n",
    "\n",
    "num_output_setEncoderAr = 128\n",
    "\n",
    "input_size_adaptation_network = num_output_setEncoderContext + num_output_setEncoderAr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP model for regression with 5 hidden layers\n",
    "num_layers_MLPRegressor = 4         # except the last layer - to be used for number of gamma and beta parameters\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, 1)  # Output layer with one node for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP model for regression with 5 hidden layers\n",
    "class adaptiveMLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(adaptiveMLPRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, 1)  # Output layer with one node for regression\n",
    "\n",
    "    def forward(self, x_context, y_context, x_target, adaptiveNetwork):\n",
    "        globalSetEncoderContext = adaptiveNetwork.globalSetEncoderContext\n",
    "        totalAdaptiveNetworkList = adaptiveNetwork.totalAdaptiveNetworkList\n",
    "\n",
    "        embeddingContext = globalSetEncoderContext(x_context, y_context)\n",
    "        embeddingContext = embeddingContext.mean(dim=0).unsqueeze(1)\n",
    "        embeddingContext = embeddingContext.repeat(1, x_target.shape[0])\n",
    "        embeddingContext = embeddingContext.T\n",
    "        gamma_beta_dict0 = totalAdaptiveNetworkList[0](embeddingContext, x_target)\n",
    "\n",
    "        out = self.fc1(x_target)\n",
    "        out = self.relu(out)\n",
    "        out = out * gamma_beta_dict0['gamma'] + gamma_beta_dict0['beta']\n",
    "\n",
    "        # totalAdaptiveNetworkList[1](embeddingContext, out) -> gamma1, beta1\n",
    "        gamma_beta_dict1 = totalAdaptiveNetworkList[1](embeddingContext, out)\n",
    "\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = out * gamma_beta_dict1['gamma'] + gamma_beta_dict1['beta']\n",
    "\n",
    "        # totalAdaptiveNetworkList[2](embeddingContext, out) -> gamma2, beta2\n",
    "        gamma_beta_dict2 = totalAdaptiveNetworkList[2](embeddingContext, out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = out * gamma_beta_dict2['gamma'] + gamma_beta_dict2['beta']\n",
    "\n",
    "        # totalAdaptiveNetworkList[3](embeddingContext, out) -> gamma3, beta3\n",
    "        gamma_beta_dict3 = totalAdaptiveNetworkList[3](embeddingContext, out)\n",
    "\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = out * gamma_beta_dict3['gamma'] + gamma_beta_dict3['beta']\n",
    "\n",
    "        out = self.fc5(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class totalAdaptationNetwork(nn.Module):\n",
    "    def __init__(self, inpute_size_Ar, hidden_size):\n",
    "        super(totalAdaptationNetwork, self).__init__()\n",
    "        self.setEncoderArNetwork = setEncoderAr(inpute_size_Ar, hidden_size)\n",
    "        self.gammaAdaptationNetwork = gammaAdaptationNetwork(input_size_adaptation_network, hidden_size)\n",
    "        self.betaAdaptationNetwork = betaAdaptationNetwork(input_size_adaptation_network, hidden_size)\n",
    "\n",
    "    def forward(self, embeddingContext, x_target_activations):\n",
    "        embeddingAr = self.setEncoderArNetwork(x_target_activations)\n",
    "        gamma = self.gammaAdaptationNetwork(embeddingContext, embeddingAr)\n",
    "        beta = self.betaAdaptationNetwork(embeddingContext, embeddingAr)\n",
    "        return {'gamma': gamma, 'beta': beta}       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class allAdaptationNetworks(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(allAdaptationNetworks, self).__init__()\n",
    "        self.globalSetEncoderContext = setEncoderContext(hidden_size)\n",
    "        self.encoderArInputSizes = [1, 128, 128, 128]\n",
    "        self.totalAdaptiveNetworkList = torch.nn.ModuleList()\n",
    "        for size in self.encoderArInputSizes:\n",
    "            self.totalAdaptiveNetworkList.append(totalAdaptationNetwork(size, hidden_size))   # List of adaptive networks (which include ArSetEncoder, gammaAdaptaionNetwork, betaAdaptationNetwork for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class setEncoderContext(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(setEncoderContext, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, hidden_size)            # input_size = 2 (for x and y point)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc6 = nn.Linear(hidden_size, num_output_setEncoderContext)  # Output layer with one node for regression\n",
    "        \n",
    "    def forward(self, x_context, y_context):     # x_y corresponds to a single point\n",
    "        x_y = torch.cat((x_context, y_context), 1)\n",
    "        # print(x_y.shape)\n",
    "        out = self.fc1(x_y)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc6(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change\n",
    "class setEncoderAr(nn.Module):\n",
    "    def __init__(self, input_size_Ar, hidden_size):\n",
    "        super(setEncoderAr, self).__init__()\n",
    "        ###\n",
    "        self.fc1 = nn.Linear(input_size_Ar, hidden_size)            # input_size = 2 (for x and y point)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc6 = nn.Linear(hidden_size, num_output_setEncoderAr)  # Output layer with one node for regression\n",
    "        \n",
    "    def forward(self, target_activations):     # x_y corresponds to a single point\n",
    "        out = self.fc1(target_activations)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc6(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setEncoderContextNetwork = setEncoderContext(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setEncoderArNetwork = setEncoderContext(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getEmbeddingsContext(X, y):\n",
    "#     embeding = torch.concat((X, y), dim=0).flatten()\n",
    "#     # print(embeding.shape)\n",
    "#     # assert embeding.shape[0] == 40\n",
    "#     return embeding\n",
    "\n",
    "\n",
    "# def getEmbeddingsContext(X, y):\n",
    "#     # print(\"Inside getEmbeddingsContext: \", X.shape, y.shape)\n",
    "#     X_y = torch.cat((X.unsqueeze(1), y.unsqueeze(1)), dim=1)\n",
    "#     embeding = setEncoderContextNetwork(X_y)\n",
    "#     embeding = embeding.mean(dim=0)            # Average over all the points in the set, size = (1, num_output_setEncoderContext)\n",
    "#     # embeding = embeding.squeeze()           # size = (num_output_setEncoderContext)\n",
    "#     # embeding = embeding.unsqueeze(1)        # size = (num_output_setEncoderContext, 1)\n",
    "#     # print(embeding.shape)\n",
    "#     # assert embeding.shape[0] == 40\n",
    "#     return embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getEmbeddingsAr(w):\n",
    "#     embeding = setEncoderArNetwork(w)\n",
    "#     return embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gammaAdaptationNetwork(nn.Module):\n",
    "    def __init__(self, input_size_adaptation_network, hidden_size):\n",
    "        super(gammaAdaptationNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size_adaptation_network, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc6 = nn.Linear(hidden_size, num_output_adaptive)  # Output layer with one node for regression\n",
    "        \n",
    "    def forward(self, embeddingsContext, embeddingsAr):\n",
    "        # print(embeddingsContext.shape, embeddingsAr.shape)\n",
    "        embeddings = torch.cat((embeddingsContext, embeddingsAr), dim=1)\n",
    "        out = self.fc1(embeddings)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc6(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class betaAdaptationNetwork(nn.Module):\n",
    "    def __init__(self, input_size_adaptation_network, hidden_size):\n",
    "        super(betaAdaptationNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size_adaptation_network, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "        self.fc6 = nn.Linear(hidden_size, num_output_adaptive)  # Output layer with one node for regression\n",
    "        \n",
    "    def forward(self, embeddingsContext, embeddingsAr):\n",
    "        embeddings = torch.cat((embeddingsContext, embeddingsAr), dim=1)\n",
    "        out = self.fc1(embeddings)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc6(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class adaptationNetwork(nn.Module):\n",
    "#     def __init__(self, input_size_adaptation_network, hidden_size):\n",
    "#         super(adaptationNetwork, self).__init__()\n",
    "\n",
    "#         self.gammaNetProcessors, self.betaNetProcessors = torch.nn.ModuleList(), torch.nn.ModuleList()\n",
    "\n",
    "#         for i in range(num_layers_MLPRegressor):\n",
    "#             self.gammaNetProcessors.append(gammaAdaptationNetwork(input_size_adaptation_network, hidden_size))\n",
    "#             self.betaNetProcessors.append(betaAdaptationNetwork(input_size_adaptation_network, hidden_size))\n",
    "        \n",
    "#     def forward(self, X, y, w):\n",
    "#         gamma_beta_lst = []\n",
    "#         embeddingsContext = getEmbeddingsContext(X, y)\n",
    "#         for i in range(num_layers_MLPRegressor):\n",
    "#             ### Define which w\n",
    "#             embeddingsAr = getEmbeddingsAr(w[i])\n",
    "#             embeddings = torch.cat((embeddingsContext, embeddingsAr), dim=0).flatten()\n",
    "\n",
    "#             gamma_beta_dict = {'gamma': self.gammaNetProcessors[i](embeddings), 'beta': self.betaNetProcessors[i](embeddings)}\n",
    "#             gamma_beta_lst.append(gamma_beta_dict)\n",
    "#         return gamma_beta_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sine(sine_params, X, i):\n",
    "#     torch.manual_seed(0)\n",
    "#     return sine_params[i][0] * torch.sin(sine_params[i][1] * X + sine_params[i][2]) + sine_params[i][3] + torch.randn(X.shape) * 0.1\n",
    "\n",
    "def get_sine(sine_params, X, i):\n",
    "    torch.manual_seed(0)\n",
    "    return sine_params[i][0] * torch.sin(X + sine_params[i][1]) + torch.randn(X.shape) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sine_params = torch.tensor(sine_params)\n",
    "torch.manual_seed(0)\n",
    "num_datasets = 101\n",
    "num_points_per_dataset = 500\n",
    "\n",
    "A = torch.distributions.Uniform(0.5, 5).sample((num_datasets,))\n",
    "phi = torch.distributions.Uniform(0, 2 * np.pi).sample((num_datasets,))\n",
    "sine_params = torch.stack((A, phi), dim=1)\n",
    "\n",
    "# Generate synthetic data for regression\n",
    "torch.manual_seed(0)\n",
    "dataset = [None] * num_datasets  # List of datasets for each function\n",
    "\n",
    "for i in range(num_datasets):\n",
    "    # upper_bound = 2 * torch.pi / sine_params[i][1]\n",
    "    upper_bound = 2 * torch.pi\n",
    "    X = torch.linspace(0, upper_bound, num_points_per_dataset)\n",
    "    y = get_sine(sine_params, X, i)  # Sine function with noise\n",
    "    X_y = torch.cat((X.unsqueeze(1), y.unsqueeze(1)), 1)\n",
    "    dataset[i] = X_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 15 sine functions and adjust margins\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.plot(dataset[i][:, 0].cpu(), dataset[i][:, 1].cpu())\n",
    "    plt.title('Sine function {}'.format(i))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset):  101\n",
      "len(dataset[0]):  500\n",
      "dataset[0].type():  torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "print('len(dataset): ', len(dataset))\n",
    "print('len(dataset[0]): ', len(dataset[0]))\n",
    "print('dataset[0].type(): ', dataset[0].type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdataset = []\n",
    "num_datapoints_per_small_task = 50      # this is the number of datapoints we will use for each iteration of meta learning\n",
    "\n",
    "for i in range(0, len(dataset)):\n",
    "    torch.manual_seed(i)\n",
    "    num_elements = dataset[i].shape[0]\n",
    "    permuted_indices = torch.randperm(num_elements) # Generate a random permutation of indices\n",
    "    dataset[i] = dataset[i][permuted_indices]\n",
    "\n",
    "    # i=0 case will be used for training of main network, therefore, kept intact. Others will be used for meta learning and testing, therefore devided in chunks of 50\n",
    "    if i>=1:\n",
    "        for j in range(int(dataset[i].shape[0] / num_datapoints_per_small_task)):\n",
    "            newdataset.append(dataset[i][j*num_datapoints_per_small_task:(j+1)*num_datapoints_per_small_task,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(newdataset):  1000\n",
      "len(newdataset[0]):  50\n"
     ]
    }
   ],
   "source": [
    "print('len(newdataset): ', len(newdataset))\n",
    "print('len(newdataset[0]): ', len(newdataset[0]))\n",
    "# print('newdataset[0]: ', newdataset[0])\n",
    "# print('newdataset[1]: ', newdataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split for meta learning\n",
    "random.seed(0)\n",
    "train_dataset = newdataset[:int(len(newdataset)*0.8)]\n",
    "random.shuffle(train_dataset)       # Shuffle the dataset in-place\n",
    "test_dataset = newdataset[int(len(newdataset)*0.8):]\n",
    "random.shuffle(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP regression baseModel with 5 hidden layers trained and saved!\n"
     ]
    }
   ],
   "source": [
    "# Training base model\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1  # One input feature\n",
    "learning_rate_base_model = 0.01\n",
    "num_epochs_base_model = 2\n",
    "split_ratio_base_model = 0.8    # for training and validation of base model\n",
    "\n",
    "# Initialize the MLP regression baseModel\n",
    "baseModel = MLPRegressor(input_size, hidden_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "optimizer = optim.Adam(baseModel.parameters(), lr=learning_rate_base_model)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs_base_model):\n",
    "    # Forward pass\n",
    "    outputs = baseModel(dataset[0][:int(num_points_per_dataset*split_ratio_base_model), 0].unsqueeze(1))\n",
    "    loss = criterion(outputs, dataset[0][:int(num_points_per_dataset*split_ratio_base_model), 1].unsqueeze(1))\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs_base_model}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    # predicted = baseModel(X)\n",
    "    predicted = baseModel(dataset[0][int(num_points_per_dataset*split_ratio_base_model):, 0].unsqueeze(1))\n",
    "\n",
    "# print(\"Predicted values:\")\n",
    "# print(predicted)\n",
    "\n",
    "# Save the trained baseModel\n",
    "torch.save(baseModel.state_dict(), 'checkpoints/baseModel.ckpt')\n",
    "print('MLP regression baseModel with 5 hidden layers trained and saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot x and y\n",
    "plt.plot(dataset[0][int(num_points_per_dataset*split_ratio_base_model):, 0].unsqueeze(1).cpu(), dataset[0][int(num_points_per_dataset*split_ratio_base_model):, 1].unsqueeze(1).cpu(), 'ro', label='Original data')\n",
    "plt.plot(dataset[0][int(num_points_per_dataset*split_ratio_base_model):, 0].unsqueeze(1).cpu(), predicted.cpu(), 'bo', label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_base_models_params(adaptiveMLPRegModel):\n",
    "#     dicts = torch.load('checkpoints/baseModel.ckpt')\n",
    "#     adaptiveMLPRegModel.fc1.weight = nn.Parameter(dicts['fc1.weight'])\n",
    "#     adaptiveMLPRegModel.fc1.bias = nn.Parameter(dicts['fc1.bias'])\n",
    "#     adaptiveMLPRegModel.fc2.weight = nn.Parameter(dicts['fc2.weight'])\n",
    "#     adaptiveMLPRegModel.fc2.bias = nn.Parameter(dicts['fc2.bias'])\n",
    "#     adaptiveMLPRegModel.fc3.weight = nn.Parameter(dicts['fc3.weight'])\n",
    "#     adaptiveMLPRegModel.fc3.bias = nn.Parameter(dicts['fc3.bias'])\n",
    "#     adaptiveMLPRegModel.fc4.weight = nn.Parameter(dicts['fc4.weight'])\n",
    "#     adaptiveMLPRegModel.fc4.bias = nn.Parameter(dicts['fc4.bias'])\n",
    "#     adaptiveMLPRegModel.fc5.weight = nn.Parameter(dicts['fc5.weight'])\n",
    "#     adaptiveMLPRegModel.fc5.bias = nn.Parameter(dicts['fc5.bias'])\n",
    "#     return adaptiveMLPRegModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Train Loss: 4.980738020259887\n",
      "Epoch [1/2], Test Loss: 5.354778202138841\n",
      "Epoch [2/2], Train Loss: 4.982230308074504\n",
      "Epoch [2/2], Test Loss: 5.354851996339858\n",
      "adaptiveMLPRegModel trained and saved!\n"
     ]
    }
   ],
   "source": [
    "# Training meta model\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate_adaptive_network = 0.001\n",
    "num_epochs_adaptive_network = 2\n",
    "\n",
    "# Initialize the MLP regression baseModel\n",
    "adaptiveMLPRegModel = adaptiveMLPRegressor(input_size, hidden_size)\n",
    "\n",
    "# load the trained baseModel\n",
    "adaptiveMLPRegModel.load_state_dict(torch.load('checkpoints/baseModel.ckpt'))\n",
    "# adaptiveMLPRegModel = load_base_models_params(adaptiveMLPRegModel)\n",
    "adaptiveNetwork = allAdaptationNetworks(hidden_size)\n",
    "\n",
    "# freez the weights of the baseModel\n",
    "for param in adaptiveMLPRegModel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "optimizer = optim.Adam(adaptiveNetwork.parameters(), lr=learning_rate_adaptive_network)\n",
    "\n",
    "train_loss_lst = []\n",
    "test_loss_lst = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs_adaptive_network):\n",
    "    loss_sum = 0\n",
    "    for training_task in train_dataset:\n",
    "        context = training_task[:context_size]\n",
    "        target_x = training_task[context_size:, 0].unsqueeze(1)\n",
    "        target_y = training_task[context_size:, 1].unsqueeze(1)\n",
    "\n",
    "        ### define w\n",
    "        \n",
    "        # gamma_beta_lst = adaptiveNetwork.forward(context[:, 0], context[:, 1], w)\n",
    "        x_context = context[:, 0].unsqueeze(1)\n",
    "        y_context = context[:, 1].unsqueeze(1)\n",
    "\n",
    "        predictions = adaptiveMLPRegModel(x_context, y_context, target_x, adaptiveNetwork)\n",
    "        loss = criterion(predictions, target_y)\n",
    "        loss_sum += loss.item()\n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs_adaptive_network}], Loss: {loss.item():.4f}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        train_loss_lst.append(loss_sum/len(train_dataset))\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs_adaptive_network}], Train Loss: {loss_sum/len(train_dataset)}')\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            loss_sum_test = 0\n",
    "            for test_task in test_dataset:\n",
    "                context_test = test_task[:context_size]\n",
    "                target_x_test = test_task[context_size:, 0].unsqueeze(1)\n",
    "                target_y_test = test_task[context_size:, 1].unsqueeze(1)\n",
    "                # gamma_beta_dict = adaptiveNetwork.forward(context_test[:, 0], context_test[:, 1])\n",
    "                # predictions = adaptiveMLPRegModel(target_x, gamma_beta_dict)\n",
    "                x_context_test = context_test[:, 0].unsqueeze(1)\n",
    "                y_context_test = context_test[:, 1].unsqueeze(1)\n",
    "                predictions = adaptiveMLPRegModel(x_context_test, y_context_test, target_x_test, adaptiveNetwork)\n",
    "                loss = criterion(predictions, target_y_test)\n",
    "                loss_sum_test += loss.item()\n",
    "            test_loss_lst.append(loss_sum_test/len(test_dataset))\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs_adaptive_network}], Test Loss: {loss_sum_test/len(test_dataset)}')\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        torch.save(adaptiveMLPRegModel.state_dict(), f'checkpoints/adaptiveNetwork_epoch_{epoch+1}.ckpt')\n",
    "\n",
    "# print(\"Predicted values:\")\n",
    "# print(predicted)\n",
    "\n",
    "# Save the trained adaptiveMLPRegModel\n",
    "torch.save(adaptiveMLPRegModel.state_dict(), f'checkpoints/adaptiveNetwork_epoch_{num_epochs_adaptive_network+1}.ckpt')\n",
    "print('adaptiveMLPRegModel trained and saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and test loss\n",
    "\n",
    "plt.plot(train_loss_lst, label='train loss')\n",
    "plt.plot(test_loss_lst, label='test loss')\n",
    "plt.xlabel('Num Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Train and Test Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_y = []\n",
    "input_x = []\n",
    "value_y = []\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    for test_task in test_dataset:\n",
    "        context = test_task[:context_size]\n",
    "        target_x = test_task[context_size:, 0].unsqueeze(1)\n",
    "        target_y = test_task[context_size:, 1].unsqueeze(1)\n",
    "        gamma_beta_dict = adaptiveNetwork.forward(context[:, 0], context[:, 1])\n",
    "        input_x.append(target_x)\n",
    "        value_y.append(target_y)\n",
    "        predictions = adaptiveMLPRegModel(target_x, gamma_beta_dict)\n",
    "        predictions_y.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(4,2, i+1)\n",
    "    plt.scatter(input_x[i*10].cpu(), value_y[i*10].cpu(), marker='o', label='Ground Truth')\n",
    "    plt.scatter(input_x[i*10].cpu(), predictions_y[i*10].cpu(), marker='x', label='Predictions')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Prediction')\n",
    "    plt.title('Function $y = A*sin(x + phi)$'.format(i))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #               [A, w, phi, B]\n",
    "# sine_params = [ [1, 1, 0, 0],           # 0\n",
    "#                 [2, 1, 0, 0],           # 1\n",
    "#                 [1, 2, 0, 0],           # 2\n",
    "#                 [0.9, 1.5, 0.3, 0.5],   # 3\n",
    "#                 [1, 1, 2, 0],         # 4\n",
    "#                 [4, 0.6, 0.9, -2],      # 5\n",
    "#                 [1.7, 4, 3.17, 0],       # 6\n",
    "#                 [3, 2, 2.19, 0.1],      # 7\n",
    "#                 [2, 1.5, 0, 0.5],       # 8\n",
    "#                 [2.5, 3, 3, 2],         # 9\n",
    "#                 [1.2, 1, 0, 0.8]]       # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class betaAdaptationNetwork(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(betaAdaptationNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "#         self.fc3 = nn.Linear(hidden_size, hidden_size)  # Output layer with one node for regression\n",
    "#         self.fc4 = nn.Linear(hidden_size, 128)  # Output layer with one node for regression\n",
    "        \n",
    "#     def forward(self, embeddings):\n",
    "#         out = self.fc1(embeddings)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc3(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc4(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sine(A, B, w, phi, x):\n",
    "#     torch.manual_seed(0)\n",
    "#     return A * torch.sin(w * x + phi) + B + torch.randn(x.shape) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# input_size = 1  # One input feature\n",
    "# hidden_size = 128  # Number of neurons in each hidden layer\n",
    "# learning_rate = 0.01\n",
    "# num_epochs = 5000\n",
    "\n",
    "# # Generate synthetic data for regression\n",
    "# torch.manual_seed(0)\n",
    "# A = 2.0\n",
    "# B = 1.0\n",
    "# w = 2.0\n",
    "# phi = 0.1\n",
    "# upper_bound = 2 * torch.pi / w\n",
    "# X = torch.linspace(0, upper_bound, 100).reshape(100, input_size)\n",
    "# y = get_sine(A, B, w, phi, X)  # Sine function with noise\n",
    "\n",
    "\n",
    "# # Initialize the MLP regression model\n",
    "# # model = MLPRegressor(input_size, hidden_size)\n",
    "# model = adaptiveMLPRegressor(input_size, hidden_size)\n",
    "\n",
    "# # Loss and optimizer\n",
    "# criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass\n",
    "#     # outputs = model(X)\n",
    "#     outputs = model(X, y)\n",
    "#     loss = criterion(outputs, y)\n",
    "\n",
    "#     # Backward pass and optimization\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# # Make predictions\n",
    "# with torch.no_grad():\n",
    "#     # predicted = model(X)\n",
    "#     predicted = model(X, y)\n",
    "\n",
    "# # print(\"Predicted values:\")\n",
    "# # print(predicted)\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), 'checkpoints/mlp_regression_model_5hiddenlayers.ckpt')\n",
    "# print('MLP regression model with 5 hidden layers trained and saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot x and y\n",
    "# plt.plot(X, y, 'ro', label='Original data')\n",
    "# plt.plot(X, predicted, 'bo', label='Fitted line')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNAPSRegression(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_tasks):\n",
    "#         super(CNAPSRegression, self).__init__()\n",
    "#         self.num_tasks = num_tasks\n",
    "#         self.task_embeddings = nn.Embedding(num_tasks, hidden_size)\n",
    "#         self.mlp = MLPRegressor(input_size + hidden_size, hidden_size)\n",
    "\n",
    "#     def forward(self, x, task):\n",
    "#         task_embedding = self.task_embeddings(task)\n",
    "#         out = torch.cat((x, task_embedding), dim=1)\n",
    "#         out = self.mlp(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size = 64):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc3(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FiLM(nn.Module):\n",
    "#     def __init__(self, input_size=3, hidden_size = 64):    # input_size = 3 corresponding to (x, y, encoder_output)\n",
    "#         super(FiLM, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.fc3 = nn.Linear(hidden_size, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc3(out)\n",
    "#         return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
